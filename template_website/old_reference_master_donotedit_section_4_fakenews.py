# -*- coding: utf-8 -*-
"""OLD_REFERENCE_Master_DoNotEdit_Section_4_FakeNews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oTxwtyVBMfkywm5e7dMdf1mOvoHE6PmG

# Section 4: Final Fake News Classification Model and Analysis

Run the below cell to get started and load our data.
"""

import math
import os
import numpy as np
from bs4 import BeautifulSoup as bs
import requests
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer
from torchtext.vocab import GloVe

import pickle

import requests, io, zipfile
# Download class resources...
r = requests.get("https://www.dropbox.com/s/2pj07qip0ei09xt/inspirit_fake_news_resources.zip?dl=1")
z = zipfile.ZipFile(io.BytesIO(r.content))
z.extractall()

basepath = '.'

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

with open(os.path.join(basepath, 'train_val_data.pkl'), 'rb') as f:
  train_data, val_data = pickle.load(f)
  
print('Number of train examples:', len(train_data))
print('Number of val examples:', len(val_data))

def get_description_from_html(html):
  soup = bs(html)
  description_tag = soup.find('meta', attrs={'name':'og:description'}) or soup.find('meta', attrs={'property':'description'}) or soup.find('meta', attrs={'name':'description'})
  if description_tag:
    description = description_tag.get('content') or ''
  else: # If there is no description, return empty string.
    description = ''
  return description

def get_descriptions_from_data(data):
  # A dictionary mapping from url to description for the websites in 
  # train_data.
  descriptions = []
  for site in tqdm(data):
    ### YOUR CODE HERE ###
    url, html, label = site
    descriptions.append(get_description_from_html(html))
    ### END CODE ###
  return descriptions

print('Getting train descriptions...')
train_descriptions = get_descriptions_from_data(train_data)
print('Getting val descriptions...')
val_descriptions = get_descriptions_from_data(val_data)

"""## Exercise 1

Before we begin, fill in *train_and_evaluate_model*, which trains and evaluates a logistic regression model given train_X, train_y, val_X, and val_y. Print train accuracy, val accuracy, confusion matrix, precision, recall, and F1-score, just as we have been doing the past few days (~7 minutes).
"""

def train_model(train_X, train_y, val_X, val_y):
  model = LogisticRegression(solver='liblinear')
  model.fit(train_X, train_y)
  
  return model


def train_and_evaluate_model(train_X, train_y, val_X, val_y):
  model = train_model(train_X, train_y, val_X, val_y)
  
  ### YOUR CODE HERE ###

  train_y_pred = model.predict(train_X)
  print('Train accuracy', accuracy_score(train_y, train_y_pred))

  val_y_pred = model.predict(val_X)
  print('Val accuracy', accuracy_score(val_y, val_y_pred))

  print('Confusion matrix:')
  print(confusion_matrix(val_y, val_y_pred))

  prf = precision_recall_fscore_support(val_y, val_y_pred)

  print('Precision:', prf[0][1])
  print('Recall:', prf[1][1])
  print('F-Score:', prf[2][1])
  
  ### END CODE HERE ###
  
  return model

"""###The Story so Far  (Read in Worksheet)

Our first approach to featurizing our data involved looking only at URLs, specifically domain name extensions. We discovered that this achieved about 60% accuracy, with many false negatives for fake news websites with an innocuous domain name extension like ".com". This accuracy wasn't great, but it gave us a baseline to improve upon.

We next moved on to keyword-based featurization, which combined the above domain name extension features with normalized counts (extracted from HTML) for a list of specific words.

## Exercise 2

Given the functions for featurizing data before and our new helper function *train_and_evaluate_model*, train and evaluate a model on top of keyword (combined with domain name extension) featurization (~8 minutes).
"""

def prepare_data(data, featurizer):
    X = []
    y = []
    for datapoint in data:
        url, html, label = datapoint
        # We convert all text in HTML to lowercase, so <p>Hello.</p> is mapped to
        # <p>hello</p>. This will help us later when we extract features from 
        # the HTML, as we will be able to rely on the HTML being lowercase.
        html = html.lower() 
        y.append(label)

        features = featurizer(url, html)

        # Gets the keys of the dictionary as descriptions, gets the values
        # as the numerical features. Don't worry about exactly what zip does!
        feature_descriptions, feature_values = zip(*features.items())

        X.append(feature_values)

    return X, y, feature_descriptions
  
# Gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword
# to lowercase).
def get_normalized_count(html, phrase):
    return math.log(1 + html.count(phrase.lower()))

# Returns a dictionary mapping from plaintext feature descriptions to numerical
# features for a (url, html) pair.
def keyword_featurizer(url, html):
    features = {}
    
    # Same as before.
    features['.com domain'] = url.endswith('.com')
    features['.org domain'] = url.endswith('.org')
    features['.net domain'] = url.endswith('.net')
    features['.info domain'] = url.endswith('.info')
    features['.org domain'] = url.endswith('.org')
    features['.biz domain'] = url.endswith('.biz')
    features['.ru domain'] = url.endswith('.ru')
    features['.co.uk domain'] = url.endswith('.co.uk')
    features['.co domain'] = url.endswith('.co')
    features['.tv domain'] = url.endswith('.tv')
    features['.news domain'] = url.endswith('.news')
    
    keywords = ['trump', 'biden', 'clinton', 'sports', 'finance']
    
    for keyword in keywords:
      features[keyword + ' keyword'] = get_normalized_count(html, keyword)
    
    return features


### YOUR CODE HERE ###
keyword_train_X, train_y, _ = prepare_data(train_data, keyword_featurizer)
keyword_val_X, val_y, _ = prepare_data(val_data, keyword_featurizer)

train_and_evaluate_model(keyword_train_X, train_y, keyword_val_X, val_y)
### END CODE HERE ###

"""We can see that we are achieving significantly better accuracy and better balance between false negatives and false positives. As expected, it looks like actually making use of the content of the HTML is useful.

Another way to take advantage of the HTML is to extract a bag-of-words (BOW) featurization from the meta description stored in the HTML. 

## Exercise 3

As before, train and evaluate this approach with our helper function *train_and_evaluate_model* (~5 minutes).
"""

vectorizer = CountVectorizer(max_features=300)

vectorizer.fit(train_descriptions)

def vectorize_data_descriptions(data_descriptions, vectorizer):
  X = vectorizer.transform(data_descriptions).todense()
  return X

### YOUR CODE HERE ###

# Note that you can use train_y and val_y from before, since these are the
# same for both the keyword approach and the BOW approach.

bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)
bow_val_X = vectorize_data_descriptions(val_descriptions, vectorizer)

train_and_evaluate_model(bow_train_X, train_y, bow_val_X, val_y)

### END CODE HERE ###

"""We can see that we are getting similar results, without necessarily using the same information as the keywords-based approach. We then asked whether we could do better by making use of word vectors, which encode the meaning of different words. We found that averaging the word vectors for words in the meta description was a useful approach. 

## Exercise 4

As before, review the approach and complete the code for training and evaluation (~5 minutes).
"""

VEC_SIZE = 300
glove = GloVe(name='6B', dim=VEC_SIZE)

# Returns word vector for word if it exists, else return None.
def get_word_vector(word):
    try:
      return glove.vectors[glove.stoi[word.lower()]].numpy()
    except KeyError:
      return None

def glove_transform_data_descriptions(descriptions):
    X = np.zeros((len(descriptions), VEC_SIZE))
    for i, description in enumerate(descriptions):
        found_words = 0.0
        description = description.strip()
        for word in description.split(): 
            vec = get_word_vector(word)
            if vec is not None:
                ### YOUR CODE HERE ###
                # Increment found_words and add vec to X[i].
                found_words += 1
                X[i] += vec
                ### END CODE HERE ###
        # We divide the sum by the number of words added, so we have the
        # average word vector.
        if found_words > 0:
            X[i] /= found_words
            
    return X
  
  
### YOUR CODE HERE ###

# Note that you can use train_y and val_y from before, since these are the
# same for both the keyword approach and the BOW approach.
  
glove_train_X = glove_transform_data_descriptions(train_descriptions)
glove_val_X = glove_transform_data_descriptions(val_descriptions)

train_and_evaluate_model(glove_train_X, train_y, glove_val_X, val_y)


### END CODE HERE ###

"""Again, solid results using a completely different approach.

## Combining Approaches (Read in Worksheet)

A natural question to ask now is whether we can combine the above featurization approaches for improved results. It turns out we can, by concatenating the feature vectors for each website produced using each of the three above approaches. Below we provide a handy helper function that takes in a list of multiple train_X produced using different featurization approaches (e.g., [keyword_train_X, bow_train_X, glove_train_X]) and combines them into *combined_train_X*. It can do this for val_X as well.

As an example, if our keyword-based approach has 15 features, our BOW approach has 300, and our GloVe approach has 300, then our combined approach has 615 features.

## Exercise 5

Complete the below code for training and evaluating the combined approach (~8 minutes).
"""

def combine_features(X_list):
  return np.concatenate(X_list, axis=1)

### YOUR CODE HERE ###
# First, produce combined_train_X and combined_val_X using 2 calls to 
# combine_features, using keyword_train_X, bow_train_X, glove_train_X
# and keyword_val_X, bow_val_X, glove_val_X from before.

combined_train_X = combine_features([keyword_train_X, bow_train_X, glove_train_X])
combined_val_X = combine_features([keyword_val_X, bow_val_X, glove_val_X])

model = train_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)

### END CODE HERE ###

"""## Exercise 6

Now, make changes to the keyword, BOW, and GloVe featurization code above to improve performance. For example, add keywords to the keyword featurization code, play around with different values for *max_features* and other parameters for [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), and try summing instead of averaging word vectors (~15 minutes).

## Exercise 7

Once you feel satisfied with your results, play around with the below demo, which scrapes a new website live and runs your trained classification model on it! The below code assumes you have run the code above and have not changed the names of any of the featurization functions. It combines the three featurization approaches. Go through the code below and make sure you understand what it is doing! Feel free to also make changes (e.g. if you only want to use two featurization approaches) (~15 minutes). Note that output is shown below the *curr_url* field.
"""

#@title Live Fake News Classification Demo { run: "auto", vertical-output: true, display-mode: "both" }
def get_data_pair(url):
  if not url.startswith('http'):
      url = 'http://' + url
  url_pretty = url
  if url_pretty.startswith('http://'):
      url_pretty = url_pretty[7:]
  if url_pretty.startswith('https://'):
      url_pretty = url_pretty[8:]
      
  # Scrape website for HTML
  response = requests.get(url, timeout=10)
  htmltext = response.text
  
  return url_pretty, htmltext

curr_url = "www.india-visa.co" #@param {type:"string"}

url, html = get_data_pair(curr_url)

# Call on the output of *keyword_featurizer* or something similar
# to transform it into a format that allows for concatenation. See
# example below.
def dict_to_features(features_dict):
  X = np.array(list(features_dict.values())).astype('float')
  X = X[np.newaxis, :]
  return X
def featurize_data_pair(url, html):
  # Approach 1.
  keyword_X = dict_to_features(keyword_featurizer(url, html))
  # Approach 2.
  description = get_description_from_html(html)
  
  bow_X = vectorize_data_descriptions([description], vectorizer)
  
  # Approach 3.
  glove_X = glove_transform_data_descriptions([description])
  
  X = combine_features([keyword_X, bow_X, glove_X])
  
  return X

curr_X = featurize_data_pair(url, html)

model = train_model(combined_train_X, train_y, combined_val_X, val_y)

curr_y = model.predict(curr_X)[0]
  
  
if curr_y < .5:
  print(curr_url, 'appears to be real.')
else:
  print(curr_url, 'appears to be fake.')

"""## Exercise 8

After playing around with your model live, do you have any ideas for how to improve it? 

1. Write down three ideas for how to improve it in your worksheet. 

2. Then, the make changes above and see what impact they have on the final model.

Once you are done, we will provide code to test your model on the test data. Note that you should only run on the test data once. When you have results on the test data, share with your instructor, as the team with the best results will be rewarded!
"""

### PUT TEST CODE HERE ###

with open(os.path.join(basepath, 'test_data.pkl'), 'rb') as f:
  test_data = pickle.load(f)
  
model = train_model(combined_train_X, train_y, combined_val_X, val_y)

print('Loading test data...')
test_X = []
for url, html, label in test_data:
  curr_X = np.array(featurize_data_pair(url, html))
  test_X.append(curr_X[0])
  
test_X = np.array(test_X)

test_y = [label for url, html, label in test_data]

print('Done loading test data...')

test_y_pred = model.predict(test_X)

print('Test accuracy', accuracy_score(test_y, test_y_pred))

print('Confusion matrix:')
print(confusion_matrix(test_y, test_y_pred))

prf = precision_recall_fscore_support(test_y, test_y_pred)

print('Precision:', prf[0][1])
print('Recall:', prf[1][1])
print('F-Score:', prf[2][1])
  
### END CODE HERE ###

"""A huge congratulations on completing the project!"""